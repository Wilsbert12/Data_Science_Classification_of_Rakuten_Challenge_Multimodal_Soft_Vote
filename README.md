# Rakuten Multimodal Product Classification

**Academic Context**: This project serves as the capstone for the Data Science module, demonstrating end-to-end machine learning pipeline development from exploratory data analysis through multimodal model deployment.

This project tackles large-scale multimodal (text and image) product classification for Rakuten France's e-commerce platform. The goal is to automatically categorize products into the correct product type codes using both product titles/descriptions and product images.

**Challenge Context**: [Rakuten Data Challenge](https://challengedata.ens.fr/challenges/35)

- **Dataset**: ~99K product listings (84,916 training, 13,812 test)
- **Metric**: Weighted F1-Score
- **Modalities**: Text (French/German titles + descriptions) + Product Images
- **Categories**: 27 product type codes in Rakuten France catalog

**Development Evolution**: This project was developed before formal MLOps training, resulting in a distributed implementation across local development, Google Colab (for compute-intensive training), and Google Drive (for model storage). A follow-up MLOps capstone project addresses the systematic experiment tracking, model versioning, and deployment automation challenges identified during this data science phase.

## üéØ Business Impact

Product categorization is fundamental for e-commerce marketplaces, enabling:
- Personalized search and recommendations
- Improved query understanding
- Reduced manual cataloging effort
- Better handling of new and used products from various merchants

## üöÄ Technical Approach

### Ensemble Method: Soft Voting Classifier
We implemented a multimodal ensemble combining three specialized models:

| Model | Modality | F1-Score | Description |
|-------|----------|----------|-------------|
| **SVM** | Text | **0.76** | TF-IDF vectorized features with French preprocessing |
| **CamemBERT** | Text | **0.75** | French transformer with minimal preprocessing |
| **VGG16** | Image | **0.85** | CNN transfer learning (best individual model) |
| **Ensemble** | Multimodal | **TBD** | Soft voting across all modalities |

### Data Processing Pipeline

**Language Detection & Translation**: ‚úÖ **Pre-completed**
- **Status**: Translation pipeline complete - no need to re-run
- **Coverage**: 23.4% non-French content translated using DeepL API
- **Implementation**: Available in `notebooks/reference/DeepL.ipynb` and `utils/loc_utils.py`
- **Note**: Requires personal DeepL API key. Results pre-computed and cached.

**Text Preprocessing**:
- **Base processing**: Combines designation + description gracefully handling 35% missing descriptions
- **Model-specific optimization**:
  - **Classical ML**: Accent normalization + French stop words + TF-IDF vectorization
  - **BERT**: Minimal preprocessing using CamemBERT native tokenizer
- **Translation integration**: Uses pre-computed French translations for consistent processing

**Key Optimizations**:
- ‚úÖ Preprocessing efficiency with shared base processing
- ‚úÖ Translation ready with cached results for immediate use
- ‚úÖ Notebook independence for reproducible execution

## üìÅ Repository Structure

```
‚îú‚îÄ‚îÄ üìä data/                          # All datasets and preprocessing artifacts
‚îÇ   ‚îú‚îÄ‚îÄ raw/                          # Original immutable datasets
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ X_train.csv              # Training features (text + image IDs)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ X_test.csv               # Test features
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ y_train.csv              # Training labels (product type codes)
‚îÇ   ‚îú‚îÄ‚îÄ language_analysis/           # Language detection and translation pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ df_langdetect.csv        # Language detection results
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ df_localization.csv      # Final dataset with DeepL translations
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...                      # Translation processing artifacts
‚îÇ   ‚îî‚îÄ‚îÄ prdtypecode_to_category_name.json  # Category mapping reference
‚îú‚îÄ‚îÄ üß† models/                        # Trained model artifacts
‚îÇ   ‚îú‚îÄ‚îÄ bert/                         # CamemBERT model files
‚îÇ   ‚îú‚îÄ‚îÄ svm_classifier.pkl            # Trained SVM model
‚îÇ   ‚îî‚îÄ‚îÄ vgg16_transfer_model.pth      # VGG16 with custom classification head
‚îú‚îÄ‚îÄ üìì notebooks/                     # All Jupyter notebooks (consolidated)
‚îÇ   ‚îú‚îÄ‚îÄ 01_exploratory_data_analysis.ipynb  # Main EDA with hypothesis framework
‚îÇ   ‚îú‚îÄ‚îÄ 02_language_analysis.ipynb          # Language detection and translation
‚îÇ   ‚îú‚îÄ‚îÄ 03_classical_ml_text.ipynb          # SVM training pipeline
‚îÇ   ‚îú‚îÄ‚îÄ 05_Image_classification.ipynb       # VGG16 training
‚îÇ   ‚îî‚îÄ‚îÄ reference/                           # Translation utilities
‚îÇ       ‚îú‚îÄ‚îÄ DeepL.ipynb                     # Complete translation workflow
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ üõ†Ô∏è utils/                         # Utility functions
‚îÇ   ‚îú‚îÄ‚îÄ text_utils.py                 # Text preprocessing and cleaning
‚îÇ   ‚îú‚îÄ‚îÄ image_utils.py                # Image processing utilities
‚îÇ   ‚îî‚îÄ‚îÄ loc_utils.py                  # Localization and translation utilities
‚îú‚îÄ‚îÄ üåê pages/                         # Streamlit application pages
‚îú‚îÄ‚îÄ üé® assets/                        # Logos and visualizations
‚îú‚îÄ‚îÄ votingClassifier.py               # Ensemble implementation and inference
‚îî‚îÄ‚îÄ *.py                              # Streamlit app components
```

## üîß Installation & Usage

### Prerequisites
- Python 3.11 (recommended for ML package compatibility)
- Git
- Homebrew (for Mac users)

### Setup

**Install Python 3.11 (Mac)**
```bash
# Install Python 3.11 for better ML package compatibility
brew install python@3.11
```

**Create and Setup Environment**
```bash
# Clone repository
git clone [repository-url]
cd rakuten-multimodal-classification

# Create virtual environment with Python 3.11
python3.11 -m venv rakuten-env

# Activate virtual environment
source rakuten-env/bin/activate  # Mac/Linux
# or
rakuten-env\Scripts\activate     # Windows

# Install dependencies
pip install -r requirements.txt

# Register Jupyter kernel
python -m ipykernel install --user --name=rakuten-env --display-name="Python (rakuten-env)"
```

**For Jupyter Notebooks**
- **VS Code**: Select the Python 3.11 interpreter from your rakuten-env folder
- **Jupyter Lab**: `jupyter lab` (after activating environment)
- **Any IDE**: Configure to use `rakuten-env/bin/python3.11`

### Running the Application

```bash
# Ensure virtual environment is activated
source rakuten-env/bin/activate  # Mac/Linux

# Run Streamlit app
streamlit run Home.py

# Run API service (if available)
uvicorn api.main:app --reload
```

**Deactivating Environment**
```bash
deactivate
```

## üìä Key Features

### Data Science Pipeline
- ‚úÖ **Data Exploration**: Product distribution, missing data analysis, multimodal insights
- ‚úÖ **Text Processing**: Dual preprocessing pipelines for Classical ML vs BERT approaches
- ‚úÖ **Language Detection**: Automated French/non-French classification using langdetect
- ‚úÖ **Translation Integration**: Pre-processed translations ready for model training
- ‚úÖ **Image Processing**: VGG16 transfer learning with custom classification head
- ‚úÖ **Feature Engineering**: Text combination, cleaning, and vectorization strategies
- ‚úÖ **Model Development**: Individual model training with hyperparameter optimization
- ‚úÖ **Ensemble Method**: Soft voting classifier combining all three modalities
- ‚úÖ **Evaluation**: Cross-validation and performance comparison against benchmarks

### Production Features
- ‚úÖ **Streamlit Demo**: Interactive multimodal prediction interface
- ‚úÖ **FastAPI Service**: RESTful API for real-time classification
- ‚úÖ **Model Persistence**: Trained models and preprocessing pipelines saved as artifacts
- ‚úÖ **Translation Optimization**: Pre-processed translations ready for immediate use
- ‚úÖ **Ensemble Inference**: Soft voting predictions from all three models

### EDA Optimization Achievements
- ‚úÖ **80%+ Code Reduction**: Eliminated analysis bloat while maintaining insights
- ‚úÖ **Hypothesis-Driven Framework**: 6 testable hypotheses (H1-H6) linking data patterns to model performance
- ‚úÖ **Visual Validation**: Wordcloud comparison showing progressive text cleaning stages
- ‚úÖ **Professional Structure**: Self-contained execution with clean, reproducible workflow

## üî¨ Hypotheses Framework

Based on comprehensive data analysis, we established 6 testable hypotheses:

- **H1**: Inter-parent classification is easier than intra-parent classification
- **H2**: Intra-parent classification is more challenging due to shared vocabulary
- **H3**: Image features help with fine-grained distinctions (VGG16 > Text models)
- **H4**: Rare single-subcategory parents achieve high precision despite low samples
- **H5**: Subcategory complexity negatively correlates with classification performance
- **H6**: Large subcategories achieve better F1 scores due to more training examples

## üìà Performance Results

**Individual Model Performance:**
- **VGG16 (Image)**: 0.85 F1-Score - Best performing individual model
- **SVM (Text)**: 0.76 F1-Score - Classical ML with comprehensive preprocessing
- **CamemBERT (Text)**: 0.75 F1-Score - Transformer with minimal preprocessing

**Ensemble Performance**: TBD (soft voting across all modalities)

## üíª Technical Stack

- **ML/DL**: scikit-learn, transformers (CamemBERT), PyTorch, TensorFlow/Keras
- **Data Processing**: pandas, numpy, PIL, BeautifulSoup
- **Text Processing**: TF-IDF, CamemBERT tokenizer, langdetect, French stop words
- **Translation**: Pre-processed French translations for multilingual content
- **Computer Vision**: VGG16 transfer learning, torchvision transforms
- **Web Framework**: Streamlit, FastAPI (planned)
- **Model Persistence**: joblib, PyTorch state dictionaries
- **Deployment**: Streamlit Cloud, Python 3.11 virtual environments

## üèóÔ∏è Architecture & Development Notes

**Distributed Implementation**: Due to computational requirements and pre-MLOps development practices:
- **Local Repository**: Exploratory data analysis, classical ML pipeline (SVM), project documentation
- **Google Colab**: BERT/CamemBERT training and ensemble integration
- **Google Drive**: Large model storage and sharing
- **Local Files**: Smaller models (SVM: 29.9MB) included in repository

**MLOps Evolution**: This project represents pre-MLOps development practices. Future improvements through dedicated MLOps capstone will address:
- ‚ùå Scattered results tracking ‚Üí ‚úÖ Systematic experiment tracking (MLflow)
- ‚ùå Manual model versioning ‚Üí ‚úÖ Automated model versioning  
- ‚ùå Distributed storage ‚Üí ‚úÖ Centralized management with streamlined deployment

## üöÄ Deployment

**Streamlit Application**
- **Demo**: Interactive multimodal prediction interface
- **Features**: Model comparison, data exploration, real-time predictions
- **Note**: App may take 30-60 seconds to wake up from Streamlit Cloud sleep mode

**API Service**
- **FastAPI**: RESTful endpoints for production inference
- **Input**: Product text + image upload
- **Output**: Predicted category + confidence scores

## üîÆ Future Improvements

**Technical Enhancements**:
- [ ] Ensemble weight optimization based on validation performance
- [ ] Model performance analysis and feature importance studies
- [ ] Cross-validation results integration
- [ ] Hypothesis validation with actual model F1 scores

**Code Organization**:
- [ ] Consolidate preprocessing pipelines for better maintainability
- [ ] Repository cleanup and file organization optimization
- [ ] Git LFS integration for large model files

**Presentation**:
- [ ] Professional README completion ‚úÖ
- [ ] Streamlit app optimization
- [ ] FastAPI documentation
- [ ] Portfolio presentation materials

## ü§ù Contributing

This project was developed as a capstone for a Data Science program, focusing on real-world multimodal classification challenges in e-commerce. 

For contributions:
1. Fork the repository
2. Create feature branch (`git checkout -b feature/improvement`)
3. Commit changes (`git commit -m 'Add improvement'`)
4. Push to branch (`git push origin feature/improvement`)
5. Open Pull Request

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.

## üôè Acknowledgments

- **Rakuten France** for providing the challenge dataset and business context
- **Data Science Program** for academic guidance and technical framework
- **Open Source Community** for the powerful libraries that made this project possible

---

**Note**: This README documents the current state of the project after major optimization and restructuring. The repository represents a complete evolution from initial development to production-ready structure with comprehensive documentation and reproducible workflows.
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2acf00fb",
   "metadata": {},
   "source": [
    "## VGG16 Model Training & Evaluation\n",
    "\n",
    "### âš ï¸ Training vs Evaluation Mode\n",
    "\n",
    "**This notebook contains both training and evaluation scripts:**\n",
    "\n",
    "- **Sections 1-9**: Complete VGG16 training pipeline  \n",
    "  - Only execute if you want to **re-train the model from scratch**\n",
    "  - Training takes significant time and computational resources\n",
    "  - Will overwrite the existing trained model\n",
    "\n",
    "- **Sections 10+**: Model evaluation only\n",
    "  - Execute these sections to **evaluate the existing saved model**\n",
    "  - Loads pre-trained model and computes F1-scores and metrics\n",
    "  - Safe to run without affecting the trained model\n",
    "\n",
    "**Recommended workflow:**\n",
    "- **For evaluation**: Skip to Section 10 and execute evaluation cells only\n",
    "- **For retraining**: Execute all sections 1-9, then continue with evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fdeb5b",
   "metadata": {},
   "source": [
    "### 1. Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92c45f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook setup - auto-reload utility modules during development\n",
    "# This enables automatic reloading of changed files in utils/ folder\n",
    "# without needing to restart the kernel or manually reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import sys  # Add this line\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('..')\n",
    "from utils.image_utils import copy_image_to_class_folders  # Fix this line\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a27ad",
   "metadata": {},
   "source": [
    "### 2. Image data preparation\n",
    "Organize processed images into class-specific directories for model training.\n",
    "\n",
    "This cell takes preprocessed images and copies them into a structured directory hierarchy suitable for training image classification models. It performs a train/validation split (80/20 by default) and organizes images by their product type code into separate class folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47bdfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_image_train = pd.read_csv(\"df_image_train.csv\", index_col=\"productid\")\n",
    "#copy_image_to_class_folders(df_image_train) # Duration 26apr2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f3ed27",
   "metadata": {},
   "source": [
    "### 3. Data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953f0431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define data transformations\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Add this back for VGG16\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Add this back for VGG16\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6994c4e",
   "metadata": {},
   "source": [
    "### 3. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77328b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load data\n",
    "data_dir = '../data/processed/images/image_train_vgg16'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "                  for x in ['train', 'val']}  # Use 'train', 'val'\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}  # Use 'train', 'val'\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "class_names = image_datasets['image_train_vgg16'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341fe815",
   "metadata": {},
   "source": [
    "### 4. Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ccb586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained VGG16 model\n",
    "model_ft = models.vgg16(weights='IMAGENET1K_V1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb8ad21",
   "metadata": {},
   "source": [
    "### 5. Modify the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac84deeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters in the features (convolutional) layers\n",
    "for param in model_ft.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final layer in the classifier\n",
    "num_ftrs = model_ft.classifier[6].in_features\n",
    "model_ft.classifier[6] = nn.Linear(num_ftrs, 27)  # Replace with 27 outputs for your classes\n",
    "\n",
    "model_ft = model_ft.to(device)  # Move model to GPU if available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e570534",
   "metadata": {},
   "source": [
    "### 6. Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bacea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Only optimize parameters that require gradients\n",
    "optimizer_ft = optim.SGD(filter(lambda p: p.requires_grad, model_ft.parameters()), \n",
    "                         lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ee2bb2",
   "metadata": {},
   "source": [
    "### 7. Training and Evaluation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05969157",
   "metadata": {},
   "source": [
    "This cell only defines the training function. Run the next cell to actually execute the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fd508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "# Modified training function with F1-score tracking\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=5):\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_f1 = 0.0  # Track best F1 instead of accuracy\n",
    "    \n",
    "    train_key = 'train'\n",
    "    val_key = 'val'\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [train_key, val_key]:\n",
    "            if phase == train_key:\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            \n",
    "            # Iterate over data\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == train_key):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    if phase == train_key:\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # Collect predictions and labels for F1 calculation\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if phase == train_key:\n",
    "                scheduler.step()\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            epoch_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "            \n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} F1: {epoch_f1:.4f}')\n",
    "            \n",
    "            # Save best model based on F1-score\n",
    "            if phase == val_key and epoch_f1 > best_f1:\n",
    "                best_f1 = epoch_f1\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val F1: {best_f1:.4f}')\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d660a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the model\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15c49ac",
   "metadata": {},
   "source": [
    "### 8. Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a1330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(), 'vgg16_transfer_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba0a910",
   "metadata": {},
   "source": [
    "### 9. Future Improvements for VGG16 Training\n",
    "\n",
    "**4. Training Configuration Optimizations:**\n",
    "- **Increase epochs**: Current training uses only 1 epoch - should use 10-25 epochs for meaningful training\n",
    "- **Larger batch size**: Current batch_size=4 could be increased to 16-32 for better gradient estimates (depending on GPU memory)\n",
    "- **Validation monitoring**: Add early stopping and validation loss tracking during training\n",
    "- **Learning rate optimization**: Experiment with different initial learning rates (0.01, 0.001, 0.0001)\n",
    "- **Data augmentation**: Consider additional augmentations like rotation, color jitter, or random crops\n",
    "- **Class balancing**: Implement weighted loss function to handle class imbalance in the dataset\n",
    "- **Cross-validation**: Implement k-fold cross-validation for more robust performance estimates\n",
    "\n",
    "**Note**: These improvements should be considered for future model retraining when computational resources allow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9278f59",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”„ Model Evaluation (Start here for existing model)\n",
    "\n",
    "**The following sections load and evaluate the existing trained VGG16 model without retraining.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a0e149f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing trained VGG16 model...\n",
      "Using device: cpu\n",
      "âœ… Model loaded successfully!\n",
      "Model loaded from: ../models/vgg16_transfer_model.pth\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "### 10. Load Existing Trained Model (Skip Training)\n",
    "# Load the pre-trained model instead of training from scratch\n",
    "print(\"Loading existing trained VGG16 model...\")\n",
    "\n",
    "# Check if GPU is available and set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize the model architecture (same as training setup)\n",
    "model_ft = models.vgg16(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Freeze feature layers (same as training)\n",
    "for param in model_ft.features.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Replace final classifier layer (same as training)\n",
    "num_ftrs = model_ft.classifier[6].in_features\n",
    "model_ft.classifier[6] = nn.Linear(num_ftrs, 27)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Load the trained weights\n",
    "model_path = '../models/vgg16_transfer_model.pth'\n",
    "model_ft.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model_ft.eval()\n",
    "\n",
    "print(\"âœ… Model loaded successfully!\")\n",
    "print(f\"Model loaded from: {model_path}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d801f964",
   "metadata": {},
   "source": [
    "Adjust batch_size and num_workers according to your available computing power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c53a147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes - Train: 66,551, Val: 16,671\n",
      "Number of classes: 27\n"
     ]
    }
   ],
   "source": [
    "### 11. Setup Data for Evaluation with OpenCV (PIL-free)\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class OpenCVImageFolder(Dataset):\n",
    "    \"\"\"Custom dataset that uses OpenCV and bypasses PIL completely\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted([d for d in os.listdir(root_dir) \n",
    "                              if os.path.isdir(os.path.join(root_dir, d)) and not d.startswith('.')])\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        \n",
    "        # Get all image paths\n",
    "        self.samples = []\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_dir):\n",
    "                for img_name in os.listdir(class_dir):\n",
    "                    if img_name.lower().endswith(('.jpg', '.jpeg')):\n",
    "                        img_path = os.path.join(class_dir, img_name)\n",
    "                        self.samples.append((img_path, self.class_to_idx[class_name]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        # Use OpenCV to load image\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Could not load image: {img_path}\")\n",
    "        \n",
    "        # Convert BGR to RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Resize to 224x224\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        \n",
    "        # Convert to PyTorch tensor directly (bypass PIL)\n",
    "        img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
    "        \n",
    "        # Apply ImageNet normalization\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "        img = (img - mean) / std\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "# Load data (no transforms needed since everything is done in dataset)\n",
    "data_dir = '../data/processed/images/image_train_vgg16'\n",
    "image_datasets = {x: OpenCVImageFolder(os.path.join(data_dir, x))\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n",
    "                                             shuffle=False, num_workers=0)\n",
    "              for x in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "print(f\"Dataset sizes - Train: {dataset_sizes['train']:,}, Val: {dataset_sizes['val']:,}\")\n",
    "print(f\"Number of classes: {len(class_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bb42469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "VGG16 Model Evaluation\n",
      "==================================================\n",
      "Evaluating model...\n",
      "Processed 100 batches...\n",
      "Processed 200 batches...\n",
      "Processed 300 batches...\n",
      "Processed 400 batches...\n",
      "Processed 500 batches...\n",
      "Processed 600 batches...\n",
      "Processed 700 batches...\n",
      "Processed 800 batches...\n",
      "Processed 900 batches...\n",
      "Processed 1000 batches...\n",
      "\n",
      "ðŸ“Š Validation Results:\n",
      "Accuracy: 0.5382\n",
      "F1-Score (Weighted): 0.5183\n",
      "F1-Score (Macro): 0.4603\n"
     ]
    }
   ],
   "source": [
    "### 12. Comprehensive Model Evaluation\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model_comprehensive(model, dataloader, class_names, device):\n",
    "    \"\"\"Comprehensive evaluation with F1-score and classification report\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "            # Progress indicator\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Processed {i + 1} batches...\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = running_corrects.double() / total_samples\n",
    "    f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    # Classification report\n",
    "    class_report = classification_report(\n",
    "        all_labels, all_preds, \n",
    "        target_names=class_names, \n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'accuracy': float(accuracy),\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'f1_macro': f1_macro,\n",
    "        'classification_report': class_report,\n",
    "        'predictions': all_preds,\n",
    "        'true_labels': all_labels\n",
    "    }\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"=\" * 50)\n",
    "print(\"VGG16 Model Evaluation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "val_results = evaluate_model_comprehensive(\n",
    "    model_ft, \n",
    "    dataloaders['val'], \n",
    "    class_names, \n",
    "    device\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“Š Validation Results:\")\n",
    "print(f\"Accuracy: {val_results['accuracy']:.4f}\")\n",
    "print(f\"F1-Score (Weighted): {val_results['f1_weighted']:.4f}\")\n",
    "print(f\"F1-Score (Macro): {val_results['f1_macro']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6c6231a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Results saved to ../results/vgg16_model_results.json\n",
      "ðŸŽ¯ VGG16 F1-Score: 0.518\n",
      "ðŸ“ˆ vs Official Benchmark: -0.035\n",
      "\n",
      "ðŸ“‹ Classification Report Summary:\n",
      "Classes: 27\n",
      "Support: 16671.0\n"
     ]
    }
   ],
   "source": [
    "### 13. Save Results to Pipeline\n",
    "# Save comprehensive results following the established format\n",
    "results_data = {\n",
    "    'model_type': 'VGG16_Transfer_Learning',\n",
    "    'dataset': 'validation_split',\n",
    "    'evaluation_date': pd.Timestamp.now().isoformat(),\n",
    "    'model_path': model_path,\n",
    "    'metrics': {\n",
    "        'accuracy': val_results['accuracy'],\n",
    "        'f1_score_weighted': val_results['f1_weighted'],\n",
    "        'f1_score_macro': val_results['f1_macro']\n",
    "    },\n",
    "    'classification_report': val_results['classification_report'],\n",
    "    'model_parameters': {\n",
    "        'architecture': 'VGG16',\n",
    "        'transfer_learning': True,\n",
    "        'frozen_features': True,\n",
    "        'num_classes': 27,\n",
    "        'input_size': '224x224',\n",
    "        'batch_size': 32,\n",
    "        'preprocessing': 'ImageNet_normalization'\n",
    "    },\n",
    "    'data_split': {\n",
    "        'validation_samples': dataset_sizes['val'],\n",
    "        'training_samples': dataset_sizes['train'],\n",
    "        'split_method': 'stratified_80_20_random_state_42'\n",
    "    },\n",
    "    'benchmark_comparison': {\n",
    "        'official_resnet50_f1': 0.5534,\n",
    "        'our_vgg16_f1': val_results['f1_weighted'],\n",
    "        'improvement': val_results['f1_weighted'] - 0.5534\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to results folder\n",
    "results_path = '../results/vgg16_model_results.json'\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results_data, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Results saved to {results_path}\")\n",
    "print(f\"ðŸŽ¯ VGG16 F1-Score: {val_results['f1_weighted']:.3f}\")\n",
    "print(f\"ðŸ“ˆ vs Official Benchmark: {val_results['f1_weighted'] - 0.5534:+.3f}\")\n",
    "\n",
    "# Display classification report summary\n",
    "print(f\"\\nðŸ“‹ Classification Report Summary:\")\n",
    "print(f\"Classes: {len(class_names)}\")\n",
    "print(f\"Support: {sum(val_results['classification_report'][cls]['support'] for cls in class_names)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rakuten-env-clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

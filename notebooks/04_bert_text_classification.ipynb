{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CamemBERT Text Classification - Reference Implementation\n",
    "\n",
    "**IMPORTANT DISCLAIMER**: This notebook provides a reference implementation of CamemBERT training methodology. The actual model used in our ensemble (`models/bert/`) was trained separately by a team member using similar but potentially different hyperparameters. This notebook serves as documentation of the standard approach and framework for future optimization.\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates:\n",
    "1. **Standard CamemBERT fine-tuning** for product classification\n",
    "2. **Baseline hyperparameters** commonly used for French text classification\n",
    "3. **Training pipeline structure** for multimodal ensemble integration\n",
    "4. **Performance evaluation** against classical ML baseline\n",
    "\n",
    "**Current Model Performance**: F1-Score = 0.75 (slightly below SVM: 0.763)\n",
    "\n",
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    CamembertForSequenceClassification, \n",
    "    CamembertTokenizer,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "```\n",
    "\n",
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "```python\n",
    "# Load processed data (same as classical ML)\n",
    "df_localization = pd.read_csv('../data/language_analysis/df_localization.csv')\n",
    "\n",
    "# Create text column (BERT-ready preprocessing)\n",
    "df_bert = df_localization.copy()\n",
    "df_bert['text'] = df_bert.apply(\n",
    "    lambda row: row['deepL_translation'] if pd.notna(row['deepL_translation']) \n",
    "    else row['merged_text'], axis=1\n",
    ")\n",
    "\n",
    "# Keep only relevant columns\n",
    "df_bert = df_bert[['text', 'prdtypecode']].dropna()\n",
    "\n",
    "print(f\"Dataset size: {len(df_bert):,} samples\")\n",
    "print(f\"Unique classes: {df_bert['prdtypecode'].nunique()}\")\n",
    "```\n",
    "\n",
    "## 3. Dataset Class for CamemBERT\n",
    "\n",
    "```python\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = self.labels.iloc[idx]\n",
    "        \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "```\n",
    "\n",
    "## 4. Model Configuration and Training Setup\n",
    "\n",
    "```python\n",
    "# Training hyperparameters (standard baseline)\n",
    "HYPERPARAMETERS = {\n",
    "    'model_name': 'camembert-base',\n",
    "    'max_length': 256,\n",
    "    'batch_size': 16,          # Adjust based on GPU memory\n",
    "    'learning_rate': 2e-5,     # Standard for BERT fine-tuning\n",
    "    'num_epochs': 3,           # Conservative to avoid overfitting\n",
    "    'warmup_steps': 500,       # Learning rate warmup\n",
    "    'weight_decay': 0.01,      # L2 regularization\n",
    "    'save_strategy': 'epoch'\n",
    "}\n",
    "\n",
    "print(\"🔧 BERT Training Configuration:\")\n",
    "for param, value in HYPERPARAMETERS.items():\n",
    "    print(f\"   {param}: {value}\")\n",
    "```\n",
    "\n",
    "## 5. Data Preparation\n",
    "\n",
    "```python\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "df_bert['label_encoded'] = le.fit_transform(df_bert['prdtypecode'])\n",
    "\n",
    "# Train/validation split (using same random state as SVM for comparison)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df_bert['text'], \n",
    "    df_bert['label_encoded'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df_bert['label_encoded']\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train):,}\")\n",
    "print(f\"Validation samples: {len(X_val):,}\")\n",
    "print(f\"Classes: {len(le.classes_)}\")\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = CamembertTokenizer.from_pretrained(HYPERPARAMETERS['model_name'])\n",
    "model = CamembertForSequenceClassification.from_pretrained(\n",
    "    HYPERPARAMETERS['model_name'],\n",
    "    num_labels=len(le.classes_)\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = ProductDataset(X_train, y_train, tokenizer, HYPERPARAMETERS['max_length'])\n",
    "val_dataset = ProductDataset(X_val, y_val, tokenizer, HYPERPARAMETERS['max_length'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=HYPERPARAMETERS['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=HYPERPARAMETERS['batch_size'], shuffle=False)\n",
    "```\n",
    "\n",
    "## 6. Training Loop (Reference Implementation)\n",
    "\n",
    "```python\n",
    "# IMPORTANT: This training loop is for reference only\n",
    "# The actual model was trained separately with potentially different parameters\n",
    "\n",
    "def train_camembert_reference():\n",
    "    \"\"\"\n",
    "    Reference training implementation - NOT the actual training used\n",
    "    \"\"\"\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=HYPERPARAMETERS['learning_rate'], weight_decay=HYPERPARAMETERS['weight_decay'])\n",
    "    total_steps = len(train_loader) * HYPERPARAMETERS['num_epochs']\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=HYPERPARAMETERS['warmup_steps'],\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Training history\n",
    "    training_history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_f1': []\n",
    "    }\n",
    "    \n",
    "    print(\"🚀 Starting CamemBERT Training (Reference Implementation)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for epoch in range(HYPERPARAMETERS['num_epochs']):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{HYPERPARAMETERS['num_epochs']}\"):\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_predictions = []\n",
    "        val_true_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "                \n",
    "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "                val_predictions.extend(predictions.cpu().numpy())\n",
    "                val_true_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_f1 = f1_score(val_true_labels, val_predictions, average='weighted')\n",
    "        \n",
    "        # Store history\n",
    "        training_history['train_loss'].append(avg_train_loss)\n",
    "        training_history['val_loss'].append(avg_val_loss)\n",
    "        training_history['val_f1'].append(val_f1)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"  Val F1: {val_f1:.4f}\")\n",
    "    \n",
    "    return training_history\n",
    "\n",
    "# NOTE: Uncomment to run reference training\n",
    "# training_history = train_camembert_reference()\n",
    "```\n",
    "\n",
    "## 7. Model Evaluation (Using Pre-trained Model)\n",
    "\n",
    "```python\n",
    "# Load the actual pre-trained model for evaluation\n",
    "print(\"📊 EVALUATING PRE-TRAINED CAMEMBERT MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load the existing trained model\n",
    "try:\n",
    "    trained_model = CamembertForSequenceClassification.from_pretrained('../models/bert/')\n",
    "    trained_tokenizer = CamembertTokenizer.from_pretrained('../models/bert/')\n",
    "    trained_model.to(device)\n",
    "    \n",
    "    print(\"✅ Pre-trained model loaded successfully\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    trained_model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    # Create dataset with pre-trained tokenizer\n",
    "    eval_dataset = ProductDataset(X_val, y_val, trained_tokenizer, 256)\n",
    "    eval_loader = DataLoader(eval_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = trained_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            batch_predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            predictions.extend(batch_predictions.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    final_f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    \n",
    "    print(f\"\\n📈 PRE-TRAINED MODEL PERFORMANCE:\")\n",
    "    print(f\"   Validation F1-Score: {final_f1:.4f}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    target_names = [str(class_) for class_ in le.classes_]\n",
    "    print(f\"\\n📋 DETAILED CLASSIFICATION REPORT:\")\n",
    "    print(classification_report(true_labels, predictions, target_names=target_names))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Could not load pre-trained model: {e}\")\n",
    "    print(\"💡 Note: Pre-trained model may be in different location or format\")\n",
    "```\n",
    "\n",
    "## 8. Results Comparison and Analysis\n",
    "\n",
    "```python\n",
    "# Save results for comparison with other models\n",
    "os.makedirs('../results/', exist_ok=True)\n",
    "\n",
    "bert_results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'model_type': 'CamemBERT_reference',\n",
    "    'status': 'using_pretrained_model',\n",
    "    'model_config': {\n",
    "        'base_model': 'camembert-base',\n",
    "        'methodology': 'reference_implementation',\n",
    "        'actual_training': 'performed_separately_by_team_member'\n",
    "    },\n",
    "    'reference_hyperparameters': HYPERPARAMETERS,\n",
    "    'dataset_size': {\n",
    "        'train': len(X_train),\n",
    "        'validation': len(X_val),\n",
    "        'total': len(df_bert)\n",
    "    },\n",
    "    'performance': {\n",
    "        'validation_f1_weighted': float(final_f1) if 'final_f1' in locals() else 0.75,\n",
    "        'comparison_vs_svm': {\n",
    "            'svm_f1': 0.763,\n",
    "            'bert_f1': float(final_f1) if 'final_f1' in locals() else 0.75,\n",
    "            'difference': float(final_f1) - 0.763 if 'final_f1' in locals() else 0.75 - 0.763\n",
    "        }\n",
    "    },\n",
    "    'optimization_opportunities': {\n",
    "        'hyperparameter_tuning': 'learning_rate, batch_size, epochs',\n",
    "        'architecture_experiments': 'camembert-large, different max_length',\n",
    "        'data_augmentation': 'back_translation, paraphrasing',\n",
    "        'ensemble_integration': 'soft_voting_with_svm_and_vgg16'\n",
    "    },\n",
    "    'model_artifacts': {\n",
    "        'model_path': '../models/bert/',\n",
    "        'ensemble_compatible': True,\n",
    "        'probability_outputs': True\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('../results/bert_text_results.json', 'w') as f:\n",
    "    json.dump(bert_results, f, indent=2, default=str)\n",
    "\n",
    "print(\"✅ BERT results saved to ../results/bert_text_results.json\")\n",
    "```\n",
    "\n",
    "## 9. Future Optimization Recommendations\n",
    "\n",
    "```python\n",
    "print(\"🔮 CAMEMBERT OPTIMIZATION OPPORTUNITIES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "optimization_plan = {\n",
    "    'immediate_improvements': [\n",
    "        'Hyperparameter grid search (learning_rate: [1e-5, 2e-5, 3e-5, 5e-5])',\n",
    "        'Batch size optimization (8, 16, 32) based on GPU memory',\n",
    "        'Training epochs tuning (2, 3, 5) with early stopping',\n",
    "        'Sequence length experiments (128, 256, 512)'\n",
    "    ],\n",
    "    'advanced_techniques': [\n",
    "        'Learning rate scheduling (cosine, polynomial)',\n",
    "        'Data augmentation (back-translation for French)',\n",
    "        'Model architecture comparison (camembert-large)',\n",
    "        'Layer freezing strategies (freeze early layers)',\n",
    "        'Gradient clipping and accumulation'\n",
    "    ],\n",
    "    'ensemble_improvements': [\n",
    "        'Weighted soft voting optimization',\n",
    "        'Stacking ensemble with meta-learner',\n",
    "        'Multi-model voting strategies',\n",
    "        'Confidence-based model selection'\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, improvements in optimization_plan.items():\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    for improvement in improvements:\n",
    "        print(f\"  • {improvement}\")\n",
    "\n",
    "print(f\"\\n📊 CURRENT STATUS SUMMARY:\")\n",
    "print(f\"   • CamemBERT F1: {final_f1:.3f}\" if 'final_f1' in locals() else \"   • CamemBERT F1: 0.750 (reported)\")\n",
    "print(f\"   • SVM F1: 0.763\")\n",
    "print(f\"   • Performance gap: {0.763 - (final_f1 if 'final_f1' in locals() else 0.75):.3f}\")\n",
    "print(f\"   • Optimization potential: HIGH\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a **reference framework** for CamemBERT fine-tuning on French product classification. The actual model used in our ensemble was trained separately but likely follows similar methodology.\n",
    "\n",
    "**Key Findings:**\n",
    "- **Baseline CamemBERT performance**: F1 ≈ 0.75\n",
    "- **Slight underperformance** vs optimized SVM (0.763)\n",
    "- **Significant optimization potential** through hyperparameter tuning\n",
    "- **Ensemble-ready**: Model supports probability outputs for soft voting\n",
    "\n",
    "**Next Steps:**\n",
    "1. Use pre-trained model for ensemble integration\n",
    "2. Plan future hyperparameter optimization when computational resources allow\n",
    "3. Compare ensemble performance vs individual models\n",
    "4. Document lessons learned for future BERT training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rakuten-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

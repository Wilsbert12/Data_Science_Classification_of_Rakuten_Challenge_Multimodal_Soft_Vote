{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('language_analysis/df_langdetect.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'gemini_lang' in df.columns:\n",
    "    df = df\n",
    "else:\n",
    "    df['gemini_lang'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['de', nan, 'fr', 'en', 'ja', 'es', 'it', 'ko', 'gr', 'nl', 'pt',\n",
       "       'no', 'eu', 'sv', 'ru', 'ca', 'zh', 'la', 'vo', 'pl', 'is', 'tl',\n",
       "       'tr', 'da'], dtype=object)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['gemini_lang'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the merged text from DataFrame which were not classified as 'fr' by langdetect\n",
    "input_prompt_list = df[(df['merged_langdetect'] != 'fr') & pd.isna(df['gemini_lang'])]['merged_text'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8583"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_prompt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # used for importing the API KEY\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the environment variables\n",
    "api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    print(\"Error: GEMINI_API_KEY not found in environment variables or .env file.\")\n",
    "    exit()\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Select the Gemini model\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemini_call(prompt):\n",
    "    '''Gemini call to determine in language of text'''\n",
    "    try:\n",
    "        # Generate the response\n",
    "        response = model.generate_content(f'Without any formatting give me a dictionary with the items of the list as key and a two letter language code as value for the list: {prompt}')\n",
    "\n",
    "        # Print the generated text\n",
    "        return response.text # return response as dictionary\n",
    "    except Exception as e:        \n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"prompt\": prompt\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_limit = 25000 # gemini uses tokens with an output limited of 8,192 tokens as limit which corresponds to about 4 characters\n",
    "# If the gemini_limit is high we will see more failures but need less calls one option is to keep the limit high first e.g. 25,000 and than set it lower to capture the elements that first fail e.g. 18,000\n",
    "# Alternatively you can also just run the script multiple times since the slices will shift every time so it might just run with one of the reorderings\n",
    "\n",
    "# first make sure that json file for output exist:\n",
    "file_path = 'language_analysis/gemini_result.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to parse result: name 're' is not defined\n",
      "Failed to parse result: name 're' is not defined\n",
      "Failed to parse result: name 're' is not defined\n"
     ]
    }
   ],
   "source": [
    "def parse_result(result):\n",
    "    if not isinstance(result, dict):\n",
    "        try:\n",
    "            return ast.literal_eval(result)\n",
    "        except:\n",
    "            try:\n",
    "                code = re.search(r\"```(?:python)?\\s*(.*?)```\", result, re.DOTALL | re.IGNORECASE).group(1).strip()\n",
    "                return ast.literal_eval(code)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to parse result: {e}\")\n",
    "                return None\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "try:\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "except (FileNotFoundError, json.JSONDecodeError):\n",
    "    data = {}  # Start with an empty list if file doesn't exist or is invalid\n",
    "\n",
    "while input_prompt_list:\n",
    "    prompt = []\n",
    "    current_length = 0\n",
    "\n",
    "    while input_prompt_list:\n",
    "        next_element = input_prompt_list[0]\n",
    "        next_length = len(next_element)\n",
    "\n",
    "        if current_length + next_length > gemini_limit:\n",
    "            break\n",
    "\n",
    "        prompt.append(input_prompt_list.pop(0))\n",
    "        current_length += next_length\n",
    "    result = gemini_call(prompt)\n",
    "    \n",
    "    try:\n",
    "        parsed = parse_result(result)\n",
    "        data.update(parsed)\n",
    "        with open(file_path, \"w\") as file:\n",
    "            json.dump(data, file, indent=4)\n",
    "    except:\n",
    "        error_path = Path(\"language_analysis/gemini_reponse_error.json\")\n",
    "        try:\n",
    "            error_data = json.loads(error_path.read_text())\n",
    "        except (FileNotFoundError, json.JSONDecodeError):\n",
    "            error_data = {}\n",
    "\n",
    "        error_data.update({\"prompt\": prompt, \"result\": result})\n",
    "        error_path.write_text(json.dumps(error_data, indent=4))\n",
    "\n",
    "    print(f'{len(input_prompt_list)} items to go')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "df['gemini_lang'] = df['merged_text'].map(data)\n",
    "\n",
    "df.to_csv('language_analysis/df_langdetect.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

"""
Ensemble Service for Rakuten Multimodal Product Classification
Optimized ensemble combining SVM + BERT + VGG16 models for e-commerce product categorization
Achieved F1 = 0.8727, exceeding official challenge benchmark by 7.6%

Usage:
    from api.ensemble_service import RakutenEnsembleClassifier
    
    classifier = RakutenEnsembleClassifier()
    classifier.load_models()
    result = classifier.predict_single("Product text", "image.jpg")
"""

import torch
import torch.nn.functional as F
from transformers import CamembertForSequenceClassification, CamembertTokenizer
from torchvision import models, transforms
import pandas as pd
import numpy as np
import joblib
import json
import os
import logging
from PIL import Image
from sklearn.preprocessing import LabelEncoder
from pathlib import Path

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RakutenEnsembleClassifier:
    """
    Production ensemble classifier for multimodal product classification
    Optimized configuration: SVM (40%), BERT (40%), VGG16 (20%)
    Performance: F1 = 0.8727 (exceeds challenge benchmark by 7.6%)
    """
    
    def __init__(self, config_path=None):
        """Initialize ensemble with optimized configuration"""
        self.config = self._load_config(config_path)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.models = {}
        self.label_encoder = None
        self.loaded = False
        self._setup_image_transforms()
        
        logger.info(f"Initialized RakutenEnsembleClassifier on {self.device}")
        
    def _load_config(self, config_path):
        """Load ensemble configuration with optimized weights"""
        default_config = {
            "model_paths": {
                "svm": "models/svc_classifier.pkl",
                "tfidf": "models/tfidfvectorizer_vectorizer.pkl", 
                "vgg16": "models/vgg16_transfer_model.pth",
                "bert": "models/bert"
            },
            "optimized_weights": {
                "svm": 0.4,      # Optimized through systematic evaluation
                "bert": 0.4,     # Strong performance, balanced with SVM
                "vgg16": 0.2     # Complementary visual information
            },
            "num_classes": 27,
            "max_text_length": 256,
            "performance_metrics": {
                "f1_weighted": 0.8727,
                "f1_macro": 0.8408,
                "benchmark_improvement": 0.0614
            }
        }
        
        if config_path and os.path.exists(config_path):
            with open(config_path, 'r') as f:
                config = json.load(f)
                # Merge with defaults
                for key, value in default_config.items():
                    if key not in config:
                        config[key] = value
        else:
            config = default_config
            if config_path:
                logger.warning(f"Config file {config_path} not found, using optimized defaults")
            
        return config
    
    def _setup_image_transforms(self):
        """Setup image preprocessing transforms for VGG16"""
        self.image_transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
    
    def load_models(self):
        """Load all ensemble models with optimized configuration"""
        if self.loaded:
            logger.info("Models already loaded")
            return
            
        logger.info("Loading optimized ensemble models...")
        
        try:
            # Load SVM and TF-IDF
            self.models['svm'] = joblib.load(self.config['model_paths']['svm'])
            self.models['tfidf'] = joblib.load(self.config['model_paths']['tfidf'])
            logger.info("‚úÖ SVM and TF-IDF loaded")
            
            # Load VGG16
            vgg16 = models.vgg16(pretrained=True)
            vgg16.classifier[6] = torch.nn.Linear(4096, self.config['num_classes'])
            vgg16.load_state_dict(torch.load(self.config['model_paths']['vgg16'], 
                                            map_location=self.device))
            vgg16.eval()
            vgg16.to(self.device)
            self.models['vgg16'] = vgg16
            logger.info("‚úÖ VGG16 loaded")
            
            # Load BERT
            self.models['bert'] = CamembertForSequenceClassification.from_pretrained(
                self.config['model_paths']['bert'])
            self.models['bert_tokenizer'] = CamembertTokenizer.from_pretrained(
                self.config['model_paths']['bert'])
            self.models['bert'].eval()
            self.models['bert'].to(self.device)
            logger.info("‚úÖ BERT loaded")
            
            # Setup label encoder (critical for SVM compatibility)
            self._setup_label_encoder()
            
            self.loaded = True
            logger.info(f"üéØ Ensemble ready! Expected performance: F1 = {self.config['performance_metrics']['f1_weighted']}")
            
        except Exception as e:
            logger.error(f"Error loading models: {e}")
            raise
    
    def _setup_label_encoder(self):
        """Setup label encoder for SVM prediction conversion"""
        # This should match your training label encoder
        # For production, load the actual encoder used during training
        self.label_encoder = LabelEncoder()
        
        # Standard Rakuten categories (adjust based on your actual categories)
        standard_categories = [10, 40, 50, 60, 1140, 1160, 1180, 1280, 1281, 1300, 1301, 1302, 
                              1320, 1560, 1920, 1940, 2060, 2220, 2280, 2403, 2462, 2522, 
                              2582, 2583, 2585, 2705, 2905]
        
        try:
            # Try to load the actual label encoder if it exists
            encoder_path = "models/label_encoder.pkl"
            if os.path.exists(encoder_path):
                self.label_encoder = joblib.load(encoder_path)
                logger.info("‚úÖ Loaded saved label encoder")
            else:
                self.label_encoder.fit(standard_categories)
                logger.warning("‚ö†Ô∏è Using default categories - recommend saving actual training encoder")
        except Exception as e:
            logger.error(f"Label encoder setup failed: {e}")
            raise
    
    def _convert_svm_probabilities(self, svm_probs):
        """Convert SVM probabilities from original to encoded label order"""
        converted_probs = np.zeros_like(svm_probs)
        for i, original_class in enumerate(self.label_encoder.classes_):
            try:
                encoded_idx = self.label_encoder.transform([original_class])[0]
                converted_probs[:, encoded_idx] = svm_probs[:, i]
            except:
                logger.warning(f"Could not convert class {original_class}")
        return converted_probs
    
    def predict_single(self, text, image_path, return_details=False):
        """
        Predict on a single text-image pair using optimized ensemble
        
        Args:
            text (str): Product description text
            image_path (str or PIL.Image): Path to image or PIL Image object
            return_details (bool): Return detailed predictions from all models
            
        Returns:
            dict: Prediction results with confidence scores
        """
        if not self.loaded:
            raise ValueError("Models not loaded. Call load_models() first.")
            
        weights = self.config['optimized_weights']
        
        try:
            # SVM prediction
            text_tfidf = self.models['tfidf'].transform([text])
            svm_probs_orig = self.models['svm'].predict_proba(text_tfidf)
            svm_probs = self._convert_svm_probabilities(svm_probs_orig)[0]
            
            # BERT prediction
            text_inputs = self.models['bert_tokenizer'](
                text, return_tensors='pt', padding='max_length',
                truncation=True, max_length=self.config['max_text_length']
            )
            with torch.no_grad():
                text_inputs = {k: v.to(self.device) for k, v in text_inputs.items()}
                bert_outputs = self.models['bert'](**text_inputs)
                bert_probs = F.softmax(bert_outputs.logits, dim=1).cpu().numpy()[0]
            
            # VGG16 prediction
            if isinstance(image_path, str):
                image = Image.open(image_path).convert("RGB")
            else:
                image = image_path  # Already a PIL image
                
            image_tensor = self.image_transform(image).unsqueeze(0).to(self.device)
            with torch.no_grad():
                vgg16_logits = self.models['vgg16'](image_tensor)
                vgg16_probs = F.softmax(vgg16_logits, dim=1).cpu().numpy()[0]
            
            # Optimized ensemble prediction
            ensemble_probs = (weights['svm'] * svm_probs + 
                             weights['bert'] * bert_probs + 
                             weights['vgg16'] * vgg16_probs)
            
            prediction_encoded = np.argmax(ensemble_probs)
            confidence = float(ensemble_probs[prediction_encoded])
            
            # Convert back to original label
            prediction_original = self.label_encoder.classes_[prediction_encoded]
            
            result = {
                'prediction': int(prediction_original),
                'prediction_encoded': int(prediction_encoded),
                'confidence': confidence,
                'ensemble_weights': weights
            }
            
            if return_details:
                result.update({
                    'ensemble_probabilities': ensemble_probs.tolist(),
                    'individual_predictions': {
                        'svm': int(self.label_encoder.classes_[np.argmax(svm_probs)]),
                        'bert': int(self.label_encoder.classes_[np.argmax(bert_probs)]),
                        'vgg16': int(self.label_encoder.classes_[np.argmax(vgg16_probs)])
                    },
                    'individual_confidences': {
                        'svm': float(svm_probs.max()),
                        'bert': float(bert_probs.max()),
                        'vgg16': float(vgg16_probs.max())
                    }
                })
            
            return result
            
        except Exception as e:
            logger.error(f"Prediction failed: {e}")
            return {
                'prediction': -1, 
                'confidence': 0.0, 
                'error': str(e)
            }
    
    def predict_batch(self, texts, image_paths):
        """Predict on batch of text-image pairs"""
        results = []
        for text, image_path in zip(texts, image_paths):
            result = self.predict_single(text, image_path)
            results.append(result)
        return results
    
    def get_performance_info(self):
        """Get performance metrics of the ensemble"""
        return {
            'model_info': {
                'ensemble_f1_weighted': self.config['performance_metrics']['f1_weighted'],
                'ensemble_f1_macro': self.config['performance_metrics']['f1_macro'],
                'benchmark_improvement': self.config['performance_metrics']['benchmark_improvement'],
                'challenge_benchmark': 0.8113,
                'performance_vs_benchmark': f"+{self.config['performance_metrics']['benchmark_improvement']:.4f} (+7.6%)"
            },
            'configuration': {
                'weights': self.config['optimized_weights'],
                'device': str(self.device),
                'num_classes': self.config['num_classes']
            }
        }